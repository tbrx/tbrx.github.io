<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="description" content="">
        <meta name="author" content="">

        <!-- Note there is no responsive meta tag here -->

        <title>brooks paige &#58;&#58; UCL machine learning</title>

        <!-- Bootstrap core CSS -->
        <link href="/bootstrap-3.1.1-dist/css/bootstrap.min.css" rel="stylesheet">

        <!-- Custom styles for this template -->
        <link href="/css/non-responsive.css" rel="stylesheet">

        <style>
        .light { font-weight: 200; }
        h4 { font-size: 21px; margin-top: 1.1em; }
        .jumbotron { background-color: #DEF; }
        </style>

        <!-- HTML5 shim and Respond.js IE8 support of HTML5 elements and media queries -->
        <!--[if lt IE 9]>
          <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
          <script src="https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
        <![endif]-->
        
        <!-- syntax highlighting CSS -->
        <link rel="stylesheet" href="/css/syntax.css">

    </head>

    <body>

    <div class="container">

  <div class="row">
    <div class="col-xs-offset-2 col-xs-8">

      <h1 id="potential-msc-project-topics">Potential MSc project topics</h1>

      <p><a href="../">Back to main page</a></p>

      <p>If you are a current MSc student at UCL, please email me at <a href="mailto:b.paige@ucl.ac.uk">b.paige@ucl.ac.uk</a> if you are interested in working on the following projects, or would like to discuss some alternative (ideally, something related to one of my <a href="https://scholar.google.com/citations?user=JrFJmx0AAAAJ">publications</a>).</p>

      <h2 id="robust-active-learning">Robust active learning</h2>

      <p><strong>Active learning</strong> is the setting in which labels are collected at the same time a machine learning model is trained, and the machine learning model can help decide which points would be most informative. 
This can be useful because the data is chosen in a way such that it is expected to be useful for training the model, and allows getting similar predictive performance from fewer labels.
However, the data you have collected is then no longer independent samples from some underlying population distribution, but instead collected following this algorithmic process.
An open question is whether or not the labels selected by an active learning algorithm are then equally useful if you were then to take them and use them for training some <em>other</em> model than the one that was used to select the point.
There’s some work (e.g. [1]). which suggest that active learning can be actively harmful in such settings.</p>

      <p>Bayesian approaches to active learning use the posterior uncertainty over the model parameters to estimate the mutual information between the model parameters and the different possible labels that could be collected [2,3].
A project on this topic would involve first empirically testing to what extent this problem exists on a handful of models and test problems.
Questions include: when are the points selected by active learning generally useful for all sorts of models — and when are they not? Does going from a “more complex” to “less complex” model work better, or vice versa? Can we fix this by defining new active learning objectives which explicitly account for generalization to other models, or which are somehow “model agnostic”?</p>

      <p>[1] Practical Obstacles to Deploying Active Learning <a href="https://arxiv.org/abs/1807.04801">https://arxiv.org/abs/1807.04801</a></p>

      <p>[2] [Bayesian active learning by disagreement <a href="https://arxiv.org/abs/1112.5745">https://arxiv.org/abs/1112.5745</a></p>

      <p>[3] [Deep Bayesian active learning with image data <a href="https://arxiv.org/abs/1703.02910">https://arxiv.org/abs/1703.02910</a>.</p>

      <h2 id="blind-and-not-so-blind-source-separation">Blind and (not-so-blind) source separation</h2>

      <p>The blind source separation problem (or “cocktail party problem”) involves decomposing a combined signal — for example, of multiple people talking simultaneously — into its independent generating components — for example, the individual separate voices. Two projects could be based on methods for blind source separation, looking at datasets of recorded music.</p>

      <p>The first project would be directly based on looking into methods for “transcribing” audio, and splitting recorded music out into different parts (be it different instruments / performers, or different notes). The talk [1] on youtube does a great job of introducing classic methods and describing recent advances. There is a dataset [2] of piano recordings aligned with transcriptions that could be a good starting point, and the same group has released other such datasets as well. This would provide an opportunity to explore a recently proposed family of deep generative models, related to independent components analysis [3].</p>

      <p>The second option would be based around “multimodal” models, looking at the dataset [4] of different people playing chamber music, including both video and audio. It could be interesting to look at this using some kind of joint modeling — e.g. using multi-modal generative models [5]. The most obvious “tasks” would be seeing (a) how adding the video could improve automatic transcription quality (intuitively, it should help do things like disambiguate who is playing what when…), or (b) how adding video can help with the source separation problem. (There are also some “funny” possibilities, e.g. given a video of people playing, but with the audio muted, try to guess how it sounds….)</p>

      <p>[1] Talk by Paris Smaragdis on non-negative matrix factorization: <a href="https://www.youtube.com/watch?v=wfmpViJIjWw">https://www.youtube.com/watch?v=wfmpViJIjWw</a></p>

      <p>[2] MAESTRO dataset: <a href="https://magenta.tensorflow.org/datasets/maestro">https://magenta.tensorflow.org/datasets/maestro</a></p>

      <p>[3] Variational Autoencoders and Nonlinear ICA: A Unifying Framework, <a href="http://proceedings.mlr.press/v108/khemakhem20a">http://proceedings.mlr.press/v108/khemakhem20a</a></p>

      <p>[4] Multi-Modal Music Performance (URMP) Dataset: <a href="http://www2.ece.rochester.edu/projects/air/projects/URMP.html">http://www2.ece.rochester.edu/projects/air/projects/URMP.html</a></p>

      <p>[5] Variational Mixture-of-Experts Autoencoders for Multi-Modal Deep Generative Models: <a href="https://arxiv.org/abs/1911.03393">https://arxiv.org/abs/1911.03393</a></p>

      <h2 id="stochastic-normalizing-flows">Stochastic normalizing flows</h2>

      <p>Normalizing flows (see review paper [1]) provide a flexible means for defining complex density functions. They take a simple base distribution p(z) and transform it to a more expressive density p(x), by way of a bijective (invertible) deterministic function x = f(z) and making use of the change-of-variables formula.
However, the restriction to <em>deterministic</em> functions reduces the expressivity. One proposal is to replace it with a stochastic function.</p>

      <p>Two recent quite different papers both consider extensions with stochastic transformations. One considers instead surjective functions, focusing on generative modeling from data, and makes analogies to variational autoencoders [2]. The other aims to learning samplers for unnormalized probability distributions, and works by explicitly introducing Metropolis-Hastings steps into the flow [3]. A goal of this project is to better understand these normalizing flow variants, exploring their modeling capabilities with different choices of deterministic and stochastic functions.</p>

      <p>We can then explore other potentially novel approaches to constructing stochastic flows. One potential option could be to consider a Bayesian treatment of parameters in a deterministic normalizing flow.</p>

      <p>[1] Normalizing Flows for Probabilistic Modeling and Inference, <a href="https://arxiv.org/abs/1912.02762">https://arxiv.org/abs/1912.02762</a></p>

      <p>[2] SurVAE Flows: Surjections to Bridge the Gap between VAEs and Flows, <a href="https://proceedings.neurips.cc/paper/2020/hash/9578a63fbe545bd82cc5bbe749636af1-Abstract.html">https://proceedings.neurips.cc/paper/2020/hash/9578a63fbe545bd82cc5bbe749636af1-Abstract.html</a></p>

      <p>[3] Stochastic Normalizing Flows, <a href="https://proceedings.neurips.cc/paper/2020/hash/9578a63fbe545bd82cc5bbe749636af1-Abstract.html">https://proceedings.neurips.cc/paper/2020/hash/9578a63fbe545bd82cc5bbe749636af1-Abstract.html</a></p>

      <h2 id="generating-high-resolution-images-with-coordinate-based-neural-networks">Generating high-resolution images with coordinate-based neural networks</h2>

      <p>Most deep generative models for images model individual pixels. In contrast, coordinate-based networks learn a function f(i,j) -&gt; x_{i,j}, which maps directly from <em>pixel indices</em> i, j to the color value of the image at that location. These approaches are particular interesting because they can be evaluated at fractional coordinates for arbitrarily high resolutions! For two interesting artistic examples of this, see [1] and [2].</p>

      <p>Recent work [3] introduces a positional encoding scheme using Fourier features which seems to be able to capture high-frequency information accurately, leading to high-resolution photorealistic reconstructions of images.</p>

      <p>However, these models are still fit to only a single image! Naively, each image is encoded by a separate function, i.e. by a separate network. The goal of this project is to explore ways of developing these coordinate-based networks into generative models that can produce a range of images, without sacrificing image quality. There are a few options here: one is to consider a latent variable model, e.g. [4], as a way of introducing an additional “context” vector beyond the i, j coordinates into the network; another is to consider explicitly learning distributions over network weights, or even other deep networks which output network weights of the coordinate-based network (as in [5]).</p>

      <p>[1] <a href="https://blog.otoro.net/2016/03/25/generating-abstract-patterns-with-tensorflow/">https://blog.otoro.net/2016/03/25/generating-abstract-patterns-with-tensorflow/</a></p>

      <p>[2] <a href="https://blog.otoro.net/2016/04/01/generating-large-images-from-latent-vectors/">https://blog.otoro.net/2016/04/01/generating-large-images-from-latent-vectors/</a></p>

      <p>[3] Fourier Features Let Networks Learn High Frequency Functions in Low Dimensional Domains, <a href="https://proceedings.neurips.cc/paper/2020/hash/55053683268957697aa39fba6f231c68-Abstract.html">https://proceedings.neurips.cc/paper/2020/hash/55053683268957697aa39fba6f231c68-Abstract.html</a></p>

      <p>[4] Auto-encoding variational Bayes, <a href="https://arxiv.org/abs/1312.6114v10">https://arxiv.org/abs/1312.6114v10</a></p>

      <p>[5] Hypernetworks, <a href="https://arxiv.org/abs/1609.09106">https://arxiv.org/abs/1609.09106</a></p>

      <p>[6] CocoNet: A deep neural network for mapping pixel coordinates to color values, <a href="https://arxiv.org/abs/1805.11357">https://arxiv.org/abs/1805.11357</a></p>

    </div>
  </div>

</div>


    <!-- Bootstrap core JavaScript
    ================================================== -->
    <!-- Placed at the end of the document so the pages load faster -->
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.0/jquery.min.js"></script>
    <script src="/bootstrap-3.1.1-dist/js/bootstrap.min.js"></script>
    <script>
      (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
      })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

      ga('create', 'UA-50547851-1', 'ox.ac.uk');
      ga('send', 'pageview');

    </script>
    </body>
</html>
