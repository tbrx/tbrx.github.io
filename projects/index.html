<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="description" content="">
        <meta name="author" content="">

        <!-- Note there is no responsive meta tag here -->

        <title>brooks paige &#58;&#58; UCL machine learning</title>

        <!-- Bootstrap core CSS -->
        <link href="/bootstrap-3.1.1-dist/css/bootstrap.min.css" rel="stylesheet">

        <!-- Custom styles for this template -->
        <link href="/css/non-responsive.css" rel="stylesheet">

        <style>
        .light { font-weight: 200; }
        h4 { font-size: 21px; margin-top: 1.1em; }
        .jumbotron { background-color: #DEF; }
        </style>

        <!-- HTML5 shim and Respond.js IE8 support of HTML5 elements and media queries -->
        <!--[if lt IE 9]>
          <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
          <script src="https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
        <![endif]-->
        
        <!-- syntax highlighting CSS -->
        <link rel="stylesheet" href="/css/syntax.css">

    </head>

    <body>

    <div class="container">

  <div class="row">
    <div class="col-xs-offset-2 col-xs-8">

      <h1 id="potential-msc-project-topics">Potential MSc project topics</h1>

      <p><a href="../">Back to main page</a></p>

      <p>If you are a current MSc student at UCL, please email me at <a href="b.paige@ucl.ac.uk">b.paige@ucl.ac.uk</a>, regarding these, or to propose any other project topic (particularly if something related to one of my <a href="https://scholar.google.co.uk/citations?user=JrFJmx0AAAAJ&amp;hl=en&amp;oi=ao">publications</a>…)!</p>

      <h2 id="fine-tuning-protein-language-models-with-experimental-feedback">Fine-tuning protein language models with experimental feedback</h2>

      <p><em>(With Alex Hawkins-Hooker)</em></p>

      <p>In parallel to recent advances in large language models trained on web-scale text data, there has been considerable interest in training protein-specific ‘language’ models (PLMs) on the hundreds of millions of protein sequences available in public databases [1]. Similar to their natural language counterparts, these protein language models can be used to generate new proteins [2], or fine-tuned to perform supervised tasks such as structure prediction or property prediction. This project seeks to explore how to use fine-tuning to improve protein language model performance on one such task: the prediction of the effects of mutations on protein function. This task is of central importance in protein design for therapeutic or biotechnological applications, where it is often desirable to engineer some natural protein to improve some property (such as the affinity of an antibody for its target). The likelihoods of pretrained language models are strong zero-shot predictors of these mutation effects [3], because by learning the distribution of natural protein sequences, they learn implicit constraints on protein function that have been selected for during evolution. Where available, these predictions can be further improved by fine-tuning on datasets of experimentally determined mutation effects specific to the protein of interest [4]. However there has to date been relatively little work on whether fine-tuning on mutation effect data from multiple proteins can lead to improved zero-shot performance on proteins for which experimental data may not be available. The recent publication of the ProteinGym benchmark [5], a carefully curated set of experimental mutation effect datasets from a large number of natural proteins, provides an ideal testing ground for testing this approach. A natural starting point for the project would be to extend a fine-tuning strategy that has successfully been applied when working with mutation effect data from a single protein to this multi-protein setting, and assessing the degree of generalisation that is achieved. The project would then aim to address questions around what kinds of fine-tuning strategy most effectively promote generalisation, e.g. should we use some form of ‘prompt tuning’ or ‘instruction tuning’ by supplying or learning representations of task type (e.g. some of the mutation effect datasets specifically measure stability, while others measure other properties such as strength of binding to some target protein)? Are methods like direct preference optimisation [6] that have proved popular in fine-tuning large language models also applicable in this setting? Which classes of protein language models are most suitable (e.g. structure-conditioned vs sequence-based models) in this setting?</p>

      <p>No biological background is required for this project; an interest in language models and a curiosity about proteins would suffice! Because of the limited number of datasets in the ProteinGym benchmark, computational requirements should not be very large, so this is an opportunity to experiment with language models without vast resources.</p>

      <p>[1] Rives et al., Biological structure and function emerge from scaling unsupervised learning to 250 million protein sequences, PNAS, 2021, <a href="https://www.pnas.org/doi/10.1073/pnas.2016239118">https://www.pnas.org/doi/10.1073/pnas.2016239118</a></p>

      <p>[2] Madani et al., Large language models generate functional protein sequences across diverse families, Nature Biotechnology, 2023, <a href="https://www.nature.com/articles/s41587-022-01618-2">https://www.nature.com/articles/s41587-022-01618-2</a></p>

      <p>[3] Meier et al, Language models enable zero-shot prediction of the effects of mutations on protein function. NeurIPS, 2021. <a href="https://openreview.net/forum?id=uXc42E9ZPFs">https://openreview.net/forum?id=uXc42E9ZPFs</a></p>

      <p>[4] ProteinGym: Large–scale benchmarks for protein fitness prediction and design, NeurIPS, 2023. <a href="https://openreview.net/forum?id=URoZHqAohf">https://openreview.net/forum?id=URoZHqAohf</a></p>

      <p>[5] Notin et al., Improving protein property prediction and design with non-parametric transformers, NeurIPS, 2023. <a href="https://openreview.net/forum?id=AwzbQVuDBk">https://openreview.net/forum?id=AwzbQVuDBk</a></p>

      <p>[6] Rafailov et al., Direct preference optimization: your language model is secretly a reward model, NeurIPS, 2023. <a href="https://openreview.net/forum?id=HPuSIXJaa9">https://openreview.net/forum?id=HPuSIXJaa9</a></p>

      <h2 id="sequential-monte-carlo-approaches-for-diffusion-models-and-language-models">Sequential Monte Carlo approaches for diffusion models and language models</h2>

      <p><em>(With Alex Hawkins-Hooker and Andrea Karlova)</em></p>

      <p>Sequential Monte Carlo (SMC) methods [1] can sample from intractable distributions by sequentially sampling from simpler distributions. Current popular deep generative models including diffusions and score-based generative models [2,3], and transformers and other GPT-styla language models [3] are fundamentally sequential methods, either directly (for language models) or in terms of iterative refinement (for diffusions). Some recent work [5] has explored the use of SMC methods for ad-hoc conditioning of diffusion models to satisfy particular constraints, for example conditioning generated samples to have a particular value under an existing classifier model. We (me and two PhD students) have a few ideas of projects related to using SMC for conditional sampling of diffusion models for molecular docking, and for protein design using large language models over protein sequences. If you are interested in these topics please reach out and we can discuss further.</p>

      <p>[1] Sequential Monte Carlo Samplers <a href="https://academic.oup.com/jrsssb/article/68/3/411/7110641">https://academic.oup.com/jrsssb/article/68/3/411/7110641</a></p>

      <p>[2] Blog post introducing diffusion models: <a href="https://lilianweng.github.io/posts/2021-07-11-diffusion-models/">https://lilianweng.github.io/posts/2021-07-11-diffusion-models/</a></p>

      <p>[3] Blog post introducing score-based generative models: <a href="https://yang-song.net/blog/2021/score/">https://yang-song.net/blog/2021/score/</a></p>

      <p>[4] Attention is all you need <a href="https://arxiv.org/abs/1706.03762">https://arxiv.org/abs/1706.03762</a></p>

      <p>[5] Practical and Asymptotically Exact Conditional Sampling in Diffusion Models <a href="https://arxiv.org/abs/2306.17775">https://arxiv.org/abs/2306.17775</a></p>

      <h2 id="diffdock-pocket-enhanced-with-protein-ligand-interactions">DiffDock-Pocket enhanced with Protein-Ligand Interactions</h2>

      <p><em>(With Andrea Karlova)</em></p>

      <p>The DiffDock-Pocket is new diffusion model used for molecular docking sampling. The contemporary diffusion models allowed mainly for blind docking, thus were lacking the ability to predict the molecular pose into the specific pocket. This design inconvenience has been recently overcome by DiffDock-Pocket which incorporate prior knowledge of the docking pocket.</p>

      <p>The existing diffusion models also mostly focus on incorporating protein-ligand interaction information solely in the reverse process, and neglect the interactions in the forward process. It has been argued that this inconsistency between forward and reverse processes may impair the binding affinity of generated molecules towards target protein.</p>

      <p>The goal of this project is investigate how well is the DiffDock-Pocket model capable to structures with realistic  protein-ligand interactions and benchmark its computational  performance and potential advanteges against the existing docking algorithms.</p>

      <p>[1] Blog post introducing diffusion models: <a href="https://lilianweng.github.io/posts/2021-07-11-diffusion-models/">https://lilianweng.github.io/posts/2021-07-11-diffusion-models/</a></p>

      <p>[2] Plainer et al. 2023: DiffDock-Pocket: Diffusion for Pocket-Level Docking with SideChain Flexibility <a href="https://www.mlsb.io/papers_2023/DiffDock-Pocket_Diffusion_for_Pocket-Level_Docking_with_Sidechain_Flexibility.pdf">https://www.mlsb.io/papers_2023/DiffDock-Pocket_Diffusion_for_Pocket-Level_Docking_with_Sidechain_Flexibility.pdf</a></p>

      <h2 id="semi-supervised-active-learning-robust-active-learning">Semi-Supervised Active Learning; Robust Active Learning</h2>

      <p><strong>Active learning</strong> is the setting in which labels are collected at the same time a machine learning model is trained, and the machine learning model can help decide which points would be most informative. 
This can be useful because the data is chosen in a way such that it is expected to be useful for training the model, and allows getting similar predictive performance from fewer labels.
Bayesian approaches to active learning use the posterior uncertainty over the model parameters to estimate the mutual information between the model parameters and the different possible labels that could be collected [1,2].
There are a couple different options for projects:</p>

      <p><strong>Semi-supervised active learning:</strong>
Semi-supervised learning (e.g. [3]) is a setting in which a predictive model is trained using <em>both</em> labelled and unlabelled datapoints.
There’s some sense in which this is a natural fit for the same setting as active learning: we have a small amount of labels for a large number of instances, and we want to predict as best we can.
Curiously, there are very few examples of applying active learning to semi-supervised models ([4] is a recent exception), and I’m not sure why.
I think there could be a good opportunity to figure out how to best decide what points may be most informative in a setting where we already are leveraging the unlabelled data, which might lead to interesting new algorithms.
The project would start with a survey of the literature, and benchmarking a few examples.</p>

      <p><strong>Robustness:</strong>
The data you have collected is then no longer independent samples from some underlying population distribution, but instead collected following this algorithmic process.
An open question is whether or not the labels selected by an active learning algorithm are then equally useful if you were then to take them and use them for training some <em>other</em> model than the one that was used to select the point.
There’s some work (e.g. [5]). which suggest that active learning can be actively harmful in such settings.
A project on this topic would involve first empirically testing to what extent this problem exists on a handful of models and test problems.
Questions include: when are the points selected by active learning generally useful for all sorts of models — and when are they not? Does going from a “more complex” to “less complex” model work better, or vice versa? Can we fix this by defining new active learning objectives which explicitly account for generalization to other models, or which are somehow “model agnostic”?</p>

      <p>[1] Bayesian active learning by disagreement <a href="https://arxiv.org/abs/1112.5745">https://arxiv.org/abs/1112.5745</a></p>

      <p>[2] Deep Bayesian active learning with image data <a href="https://arxiv.org/abs/1703.02910">https://arxiv.org/abs/1703.02910</a>.</p>

      <p>[3] Semi-Supervised Learning with Deep Generative Models <a href="https://arxiv.org/abs/1406.5298">https://arxiv.org/abs/1406.5298</a></p>

      <p>[4] Semi-Supervised Active Learning with Temporal Output Discrepancy <a href="https://arxiv.org/abs/2107.14153">https://arxiv.org/abs/2107.14153</a></p>

      <p>[5] Practical Obstacles to Deploying Active Learning <a href="https://arxiv.org/abs/1807.04801">https://arxiv.org/abs/1807.04801</a></p>

      <h2 id="blind-and-not-so-blind-source-separation">Blind and (not-so-blind) source separation</h2>

      <p>The blind source separation problem (or “cocktail party problem”) involves decomposing a combined signal — for example, of multiple people talking simultaneously — into its independent generating components — for example, the individual separate voices. Two projects could be based on methods for blind source separation, looking at datasets of recorded music.</p>

      <p>The first project would be directly based on looking into methods for “transcribing” audio, and splitting recorded music out into different parts (be it different instruments / performers, or different notes). The talk [1] on youtube does a great job of introducing classic methods and describing recent advances. There is a dataset [2] of piano recordings aligned with transcriptions that could be a good starting point, and the same group has released other such datasets as well. This would provide an opportunity to explore a recently proposed family of deep generative models, related to independent components analysis [3].</p>

      <p>The second option would be based around “multimodal” models, looking at the dataset [4] of different people playing chamber music, including both video and audio. It could be interesting to look at this using some kind of joint modeling — e.g. using multi-modal generative models [5]. The most obvious “tasks” would be seeing (a) how adding the video could improve automatic transcription quality (intuitively, it should help do things like disambiguate who is playing what when…), or (b) how adding video can help with the source separation problem. (There are also some “funny” possibilities, e.g. given a video of people playing, but with the audio muted, try to guess how it sounds….)</p>

      <p>[1] Talk by Paris Smaragdis on non-negative matrix factorization: <a href="https://www.youtube.com/watch?v=wfmpViJIjWw">https://www.youtube.com/watch?v=wfmpViJIjWw</a></p>

      <p>[2] MAESTRO dataset: <a href="https://magenta.tensorflow.org/datasets/maestro">https://magenta.tensorflow.org/datasets/maestro</a></p>

      <p>[3] Variational Autoencoders and Nonlinear ICA: A Unifying Framework, <a href="http://proceedings.mlr.press/v108/khemakhem20a">http://proceedings.mlr.press/v108/khemakhem20a</a></p>

      <p>[4] Multi-Modal Music Performance (URMP) Dataset: <a href="http://www2.ece.rochester.edu/projects/air/projects/URMP.html">http://www2.ece.rochester.edu/projects/air/projects/URMP.html</a></p>

      <p>[5] Variational Mixture-of-Experts Autoencoders for Multi-Modal Deep Generative Models: <a href="https://arxiv.org/abs/1911.03393">https://arxiv.org/abs/1911.03393</a></p>

    </div>
  </div>

</div>


    <!-- Bootstrap core JavaScript
    ================================================== -->
    <!-- Placed at the end of the document so the pages load faster -->
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.0/jquery.min.js"></script>
    <script src="/bootstrap-3.1.1-dist/js/bootstrap.min.js"></script>
    <script>
      (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
      })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

      ga('create', 'UA-50547851-1', 'ox.ac.uk');
      ga('send', 'pageview');

    </script>
    </body>
</html>
