<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="description" content="">
        <meta name="author" content="">

        <!-- Note there is no responsive meta tag here -->

        <title>brooks paige &#58;&#58; UCL machine learning</title>

        <!-- Bootstrap core CSS -->
        <link href="/bootstrap-3.1.1-dist/css/bootstrap.min.css" rel="stylesheet">

        <!-- Custom styles for this template -->
        <link href="/css/non-responsive.css" rel="stylesheet">

        <style>
        .light { font-weight: 200; }
        h4 { font-size: 21px; margin-top: 1.1em; }
        .jumbotron { background-color: #DEF; }
        </style>

        <!-- HTML5 shim and Respond.js IE8 support of HTML5 elements and media queries -->
        <!--[if lt IE 9]>
          <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
          <script src="https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
        <![endif]-->
        
        <!-- syntax highlighting CSS -->
        <link rel="stylesheet" href="/css/syntax.css">

    </head>

    <body>

    <div class="container">

  <div class="row">
    <div class="col-xs-offset-2 col-xs-8">

      <h1 id="potential-msc-project-topics">Potential MSc project topics</h1>

      <p><a href="../">Back to main page</a></p>

      <p>If you are a current MSc student at UCL, please email me at <a href="mailto:b.paige@ucl.ac.uk">b.paige@ucl.ac.uk</a> if you are interested in working on the following projects, or would like to discuss some alternative (ideally, something related to one of my <a href="https://scholar.google.com/citations?user=JrFJmx0AAAAJ">publications</a>).</p>

      <h2 id="deep-neural-networks-for-density-functional-theory">Deep Neural Networks for Density Functional Theory</h2>

      <p>The Schrodinger Equation provides an elegant description of the positions of the electrons in the molecular structures.
Despite the fact that the equation has been known for nearly a century, providing the solution, which accurately characterises the wave functions, has remained a challenging task.
Various numerical approaches have been developed, such as Density Functional Theory (DFT), Quantum Monte Carlo, Moller-Plesset Perturbation Theory among others.
The computational complexity of the methods varies by a polynomial order for a fixed number of electrons in the chemical system with DFT being amongst the fastest having a cubic computational complexity.
The ability to improve the computational complexity of these algorithms can significantly speed up the discovery of new materials or new drugs.</p>

      <p>Unfortunately, due to various simplifying assumptions DFT is often innacurate.
Recent work [1,2] has applied machine learning methods to directly learn a correction factor, the <em>exchange correlation functional</em>, which captures higher-order interactions between electrons, 
or alternately has directly predicted energy solutions [3].
These approaches are fairly simple, with limited ability to generalize to new structures.
A project can focus on a few different aspects; this includes developing more appropriate architectures (including graph neural networks), as well as exploring alternative training regimes (in particular, employing meta-learning [4]) to directly address concerns of generalization.</p>

      <p>[1] Machine learning accurate exchange and correlation functionals of the electronic density <a href="https://www.nature.com/articles/s41467-020-17265-7">https://www.nature.com/articles/s41467-020-17265-7</a></p>

      <p>[2] Highly accurate and constrained density functional obtained with differentiable programming <a href="https://journals.aps.org/prb/abstract/10.1103/PhysRevB.104.L161109">https://journals.aps.org/prb/abstract/10.1103/PhysRevB.104.L161109</a></p>

      <p>[3] OrbNet: Deep learning for quantum chemistry using symmetry-adapted atomic-orbital features <a href="https://aip.scitation.org/doi/10.1063/5.0021955">https://aip.scitation.org/doi/10.1063/5.0021955</a></p>

      <p>[4] Meta-Learning in Neural Networks: A Survey <a href="https://arxiv.org/abs/2004.05439">https://arxiv.org/abs/2004.05439</a></p>

      <h2 id="semi-supervised-active-learning-robust-active-learning">Semi-Supervised Active Learning; Robust Active Learning</h2>

      <p><strong>Active learning</strong> is the setting in which labels are collected at the same time a machine learning model is trained, and the machine learning model can help decide which points would be most informative. 
This can be useful because the data is chosen in a way such that it is expected to be useful for training the model, and allows getting similar predictive performance from fewer labels.
Bayesian approaches to active learning use the posterior uncertainty over the model parameters to estimate the mutual information between the model parameters and the different possible labels that could be collected [1,2].
There are a couple different options for projects:</p>

      <p><strong>Semi-supervised active learning:</strong>
Semi-supervised learning (e.g. [3]) is a setting in which a predictive model is trained using <em>both</em> labelled and unlabelled datapoints.
There’s some sense in which this is a natural fit for the same setting as active learning: we have a small amount of labels for a large number of instances, and we want to predict as best we can.
Curiously, there are very few examples of applying active learning to semi-supervised models ([4] is a recent exception), and I’m not sure why.
I think there could be a good opportunity to figure out how to best decide what points may be most informative in a setting where we already are leveraging the unlabelled data, which might lead to interesting new algorithms.
The project would start with a survey of the literature, and benchmarking a few examples.</p>

      <p><strong>Robustness:</strong>
The data you have collected is then no longer independent samples from some underlying population distribution, but instead collected following this algorithmic process.
An open question is whether or not the labels selected by an active learning algorithm are then equally useful if you were then to take them and use them for training some <em>other</em> model than the one that was used to select the point.
There’s some work (e.g. [5]). which suggest that active learning can be actively harmful in such settings.
A project on this topic would involve first empirically testing to what extent this problem exists on a handful of models and test problems.
Questions include: when are the points selected by active learning generally useful for all sorts of models — and when are they not? Does going from a “more complex” to “less complex” model work better, or vice versa? Can we fix this by defining new active learning objectives which explicitly account for generalization to other models, or which are somehow “model agnostic”?</p>

      <p>[1] Bayesian active learning by disagreement <a href="https://arxiv.org/abs/1112.5745">https://arxiv.org/abs/1112.5745</a></p>

      <p>[2] Deep Bayesian active learning with image data <a href="https://arxiv.org/abs/1703.02910">https://arxiv.org/abs/1703.02910</a>.</p>

      <p>[3] Semi-Supervised Learning with Deep Generative Models <a href="https://arxiv.org/abs/1406.5298">https://arxiv.org/abs/1406.5298</a></p>

      <p>[4] Semi-Supervised Active Learning with Temporal Output Discrepancy <a href="https://arxiv.org/abs/2107.14153">https://arxiv.org/abs/2107.14153</a></p>

      <p>[5] Practical Obstacles to Deploying Active Learning <a href="https://arxiv.org/abs/1807.04801">https://arxiv.org/abs/1807.04801</a></p>

      <h2 id="data-driven-weather-forecasting">Data-driven weather forecasting</h2>

      <p>Most weather forecasting is currently done using physics-based models (derived from first principles), not machine learning models (learned from data).
There is a recent “challenge dataset” released, as part of the WeatherBench project [1,2]
The paper [2] includes an overview of a number of recent papers proposing deep learning approaches,
and the repo includes a suggestion of various baselines (incl. climatology models) at predicting the weather a few days out.
While perhaps the “obvious” thing to do here is to focus on developing architectures, or selecting machine learning models, which then perform well against the benchmark,
I think there are actually a number of questions here which a project could focus on.
In particular, there are issues related to direct prediction at different lead times, vs using machine learning to learn an approximation of the dynamics, which could then be iterated.
(The later has a more obvious physical interpretation, but is prone to accumulating numeric error.)
There’s also a lot of room open for developing “hybrid” approaches which incorporate both standard climate modeling and machine learning.
If you are interested in this area we can go through the paper together and discuss some options.</p>

      <p>[1] WeatherBench: A benchmark dataset for data-driven weather forecasting <a href="https://arxiv.org/abs/2002.00469">https://arxiv.org/abs/2002.00469</a></p>

      <p>[2] Github repo <a href="https://github.com/pangeo-data/WeatherBench">https://github.com/pangeo-data/WeatherBench</a></p>

      <h2 id="blind-and-not-so-blind-source-separation">Blind and (not-so-blind) source separation</h2>

      <p>The blind source separation problem (or “cocktail party problem”) involves decomposing a combined signal — for example, of multiple people talking simultaneously — into its independent generating components — for example, the individual separate voices. Two projects could be based on methods for blind source separation, looking at datasets of recorded music.</p>

      <p>The first project would be directly based on looking into methods for “transcribing” audio, and splitting recorded music out into different parts (be it different instruments / performers, or different notes). The talk [1] on youtube does a great job of introducing classic methods and describing recent advances. There is a dataset [2] of piano recordings aligned with transcriptions that could be a good starting point, and the same group has released other such datasets as well. This would provide an opportunity to explore a recently proposed family of deep generative models, related to independent components analysis [3].</p>

      <p>The second option would be based around “multimodal” models, looking at the dataset [4] of different people playing chamber music, including both video and audio. It could be interesting to look at this using some kind of joint modeling — e.g. using multi-modal generative models [5]. The most obvious “tasks” would be seeing (a) how adding the video could improve automatic transcription quality (intuitively, it should help do things like disambiguate who is playing what when…), or (b) how adding video can help with the source separation problem. (There are also some “funny” possibilities, e.g. given a video of people playing, but with the audio muted, try to guess how it sounds….)</p>

      <p>[1] Talk by Paris Smaragdis on non-negative matrix factorization: <a href="https://www.youtube.com/watch?v=wfmpViJIjWw">https://www.youtube.com/watch?v=wfmpViJIjWw</a></p>

      <p>[2] MAESTRO dataset: <a href="https://magenta.tensorflow.org/datasets/maestro">https://magenta.tensorflow.org/datasets/maestro</a></p>

      <p>[3] Variational Autoencoders and Nonlinear ICA: A Unifying Framework, <a href="http://proceedings.mlr.press/v108/khemakhem20a">http://proceedings.mlr.press/v108/khemakhem20a</a></p>

      <p>[4] Multi-Modal Music Performance (URMP) Dataset: <a href="http://www2.ece.rochester.edu/projects/air/projects/URMP.html">http://www2.ece.rochester.edu/projects/air/projects/URMP.html</a></p>

      <p>[5] Variational Mixture-of-Experts Autoencoders for Multi-Modal Deep Generative Models: <a href="https://arxiv.org/abs/1911.03393">https://arxiv.org/abs/1911.03393</a></p>

      <h2 id="generating-high-resolution-images-with-coordinate-based-neural-networks">Generating high-resolution images with coordinate-based neural networks</h2>

      <p>Most deep generative models for images model individual pixels. In contrast, coordinate-based networks learn a function f(i,j) -&gt; x_{i,j}, which maps directly from <em>pixel indices</em> i, j to the color value of the image at that location. These approaches are particular interesting because they can be evaluated at fractional coordinates for arbitrarily high resolutions! For two interesting artistic examples of this, see [1] and [2].</p>

      <p>Recent work [3] introduces a positional encoding scheme using Fourier features which seems to be able to capture high-frequency information accurately, leading to high-resolution photorealistic reconstructions of images.</p>

      <p>However, these models are still fit to only a single image! Naively, each image is encoded by a separate function, i.e. by a separate network. The goal of this project is to explore ways of developing these coordinate-based networks into generative models that can produce a range of images, without sacrificing image quality. There are a few options here: one is to consider a latent variable model, e.g. [4], as a way of introducing an additional “context” vector beyond the i, j coordinates into the network; another is to consider explicitly learning distributions over network weights, or even other deep networks which output network weights of the coordinate-based network (as in [5]).</p>

      <p>[1] <a href="https://blog.otoro.net/2016/03/25/generating-abstract-patterns-with-tensorflow/">https://blog.otoro.net/2016/03/25/generating-abstract-patterns-with-tensorflow/</a></p>

      <p>[2] <a href="https://blog.otoro.net/2016/04/01/generating-large-images-from-latent-vectors/">https://blog.otoro.net/2016/04/01/generating-large-images-from-latent-vectors/</a></p>

      <p>[3] Fourier Features Let Networks Learn High Frequency Functions in Low Dimensional Domains, <a href="https://proceedings.neurips.cc/paper/2020/hash/55053683268957697aa39fba6f231c68-Abstract.html">https://proceedings.neurips.cc/paper/2020/hash/55053683268957697aa39fba6f231c68-Abstract.html</a></p>

      <p>[4] Auto-encoding variational Bayes, <a href="https://arxiv.org/abs/1312.6114v10">https://arxiv.org/abs/1312.6114v10</a></p>

      <p>[5] Hypernetworks, <a href="https://arxiv.org/abs/1609.09106">https://arxiv.org/abs/1609.09106</a></p>

      <p>[6] CocoNet: A deep neural network for mapping pixel coordinates to color values, <a href="https://arxiv.org/abs/1805.11357">https://arxiv.org/abs/1805.11357</a></p>

    </div>
  </div>

</div>


    <!-- Bootstrap core JavaScript
    ================================================== -->
    <!-- Placed at the end of the document so the pages load faster -->
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.0/jquery.min.js"></script>
    <script src="/bootstrap-3.1.1-dist/js/bootstrap.min.js"></script>
    <script>
      (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
      })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

      ga('create', 'UA-50547851-1', 'ox.ac.uk');
      ga('send', 'pageview');

    </script>
    </body>
</html>
